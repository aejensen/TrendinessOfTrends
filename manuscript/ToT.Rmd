---
title: "Quantifying the Trendiness of Trends (or how I learned to stop worrying and love the uncertainty of changes)"
author: | 
  | Andreas Kryger Jensen and Claus Ekstrøm
  | Biostatistics, Institute of Public Health, University of Copenhagen
date: "`r format(Sys.time(), '%d %B, %Y')`"
fontsize: 11pt  
header-includes:
  - \usepackage{bm}
  - \usepackage[labelfont=bf]{caption}
  - \DeclareMathOperator*{\argsup}{arg\,sup}  
  - \DeclareMathOperator*{\argmin}{arg\,min}  
  - \DeclareMathOperator*{\E}{E}
  - \DeclareMathOperator*{\Cov}{Cov}
  - \DeclareMathOperator*{\Var}{Var}
  - \DeclareMathOperator*{\Erf}{Erf}
  - \usepackage{multirow}
  - \usepackage[amsthm,thmmarks]{ntheorem}
  - \newtheorem{definition}{Definition}
  - \newtheorem{assumption}{Assumption}
  - \newtheorem{proposition}{Proposition}
output:
  pdf_document: 
    number_sections: yes
    toc: false
bibliography: bibliography.bib    
---


\begin{abstract}
A statement often seen in the news concerning some public health outcome is that some trend has changed or been broken. Such statements are often based on longitudinal data from e.g., surveys, and the change in the trend is claimed to have occurred at the time of the latest data collection. These types of statistical assessments are very important as they may potentially influence public health decisions on a national level.

Instead we propose a statistical model using methods from Functional Data Analysis under the assumption that reality evolves in continuous time. Under this assumption we define what constitutes a trend and a change in a trend, and we introduce a probabilistic Trend Direction Index. This index has the intuitive interpretation of the probability that an underlying trend has changed direction at any given time conditional on observed data. We also define a global index of Expected Trend Instability that quantifies the expected number of times a trend changes on an interval.

We show how the Trend Direction Index and the Expected Trend Instability can be estimated from data in a Bayesian framework and give an application to development of the proportion of smokers in Denmark during the last 20 years.
\end{abstract}

\begin{center}
\textbf{Keywords:} Functional Data Analysis, Gaussian Processes, Trends, Bayesian Statistics 
\end{center}

# Introduction
This manuscript is concerned with the fundamental problem of estimating an underlying trend based on random variables observed repeatedly over time. In addition to this problem we also wish to assess points in time where it is possible that such a trend is changing. Our motivation is a recent example from the news in Denmark where it was stated that the trend in the proportion of smokers in Denmark has changed at the end of the year 2018. This statement was based on survey data collected yearly since 1998 and reported by the Danish Health Authority [@sst]. It is not immediately obvious what constitutes a trend and more so what a change in a trend is. To elucidate these questions and provide a clear specification we start from the ground by defining the problem. 

[\textbf{TODO:} Claus tilbyder at båtte sig igennem resten af introduktionen]. 

[\textbf{TODO:} What do we propose? We work in continuous time and introduce TDI and ETI informally at this point. TDI gives the local probability of the direction of $f$ and can answer the question ,,What is the probability that $f$ is increasing?''. ETI gives the expected number of changes in the direction of $f$ on an interval and can answer the question ,,Is it the first time in 20 years that $f$ has changed trend?]. 

[\textbf{TODO:} We are not doing a hypothesis test and compute a p-value. I.e., we do not calculate the probablity that an observed change in trend is different than would have been expected if there was no change in trend. We estimate the actual probability that the trend in changing.] 

\begin{figure}[htb]
\center\includegraphics{../figures/rawDataPlot}
\caption{The proportion of daily or occational smokers in Denmark during the last 20 years estimated from survey data and reported by the Danish Health Authority. [\textbf{TODO:} Mention in text why 2009 is missing]}
\label{fig:rawDataPlot}
\end{figure}

[\textbf{TODO:} Assert that both TDI and EDI are scale free. They don't tell anything about the magnitude of a trend. They should therefore always be accompanied by plots of the posterior of $df$]. 


[\textbf{TODO:} Reference some previous work]. @gottlieb2012stickiness define a stickiness coefficient for longitudinal data. Our $\text{ETI}$ has some similarities to the tracking index of @foulkes1981index.

The manuscript is structured as follows: In section \ref{sec:method} we present our statistical model based on a latent Gaussian Process formulation giving rise to analytic expressions for the Trend Direction Index and the Expected Trend Instability index conditional on observed data. We then extend this basic model to enable a data-driven adaptation to non-stationarity in the underlying dynamics. Section \ref{sec:estimation} is concerned with estimating the models parameters, and we give an application to the proportion of smokers in Denmark during the last 20 years in section \ref{sec:application}, and we conclude with a discussion.



# Methods {#sec:method}
We assume that reality evolves in continuous time $t \in \mathcal{T} \subset \mathbb{R}$ and that there exists a latent, random function $f = \left\{f(t) : t \in \mathcal{T}\right\}$ that governs the underlying evolution of some observable characteristic in a population. We are able to observe a noisy version of this latent characteristic by sampling $f$ at discrete time points according to the additive model $Y_i = f(t_i) + \varepsilon_i$ where $Y_i$ is the $i$'th observation at time $t_i \in \mathcal{T}$ and $\varepsilon_i$ is a random variable with $\E[\varepsilon_i \mid t_i] = 0$ [\textbf{TODO}: Explain why?]. Given observations of the form $(Y_i, t_i)_{i=1}^n$ we are interested in estimating the dynamical properties of the latent $f$.

The trend of $f$ is defined as its instantaneous slope given by the function $df(t) = \left(\frac{\mathrm{d}f(s)}{\mathrm{d}s}\right)(t)$, and we say that $f$ exhibits a positive trend at $t$ if $df(t) > 0$ and a negative trend at $t$ if $df(t) < 0$. A change in trend is defined as a point in time where $df$ changes sign, i.e., a positive trend becomes a negative trend meaning that $f$ goes from being increasing to becoming decreasing or vice versa. As $f$ is a random function there are no exact points in time where $df$ changes sign. This is instead a gradual and continuous concept and the assessment must be defined in terms of the probability that a change in trend is occurring. This stands in contrast to traditional change-point models which assume that there are one or more exact time points where sudden a change in a regression parameter occurs [@picard1985testing].

To quantify the probability that a trend is changing we introduce the following local probabilistic Trend Direction Index
\begin{align}
  \mathrm{TDI}(t, \delta) = P(df(t + \delta ) > 0 \mid \mathcal{F}_t)\label{eq:TCIdef}
\end{align}
where $\mathcal{F}_t$ is the $\sigma$-algebra of all available information observed up until time $t$. The value of $\mathrm{TDI}(t, \delta)$ is equal to the probability that $f$ is increasing at time $t + \delta$ given all we know about the data generating process up until time $t$. A similar definition could be given for a negative trend but that is equal to $1 - \mathrm{TDI}(t, \delta)$ and therefore redundant. The sign of $\delta$ determines whether we wish to estimate the past ($\delta \leq 0$) or forecast into the future ($\delta > 0$). Most of the examples seen in the news concerning public health outcomes is concerned with $t$ being equal to the current calendar time and $\delta = 0$. This excludes using both change-point or segmented regression models [@quandt1958estimation] as there are no observations beyond the stipulated change-point. A useful parametrization of the Trend Direction Index is $\mathrm{TDI}(\max \mathcal{T}, t - \max \mathcal{T})$ with $t \leq \max \mathcal{T}$ which conditions on the full observation period and looks back in time. In this parametrization $t = \max \mathcal{T}$ corresponds to the Trend Direction Index at the end of the observation period. A natural threshold for trendiness is $\mathrm{TDI} = 0.5$ corresponding to a situation where the trend is symmetrically distributed around zero. Depending on the application, different thresholds can be used based on external loss or utility functions.

In addition to the Trend Direction Index we also define a global index of trend instability. We say that a function $f$ is \textit{trend stable} on an interval $\mathcal{I} \subseteq \mathcal{T}$ if it remains either increasing or decreasing so that its trend maintains its sign. As a quantification of the trend instability of a function $f$ we propose to use the expected number of times that the function changes direction on the interval. This is equivaltn to counting the number of zero crossings by $df$ on $\mathcal{I}$. Informally at this point, we define the Expected Trend Instability as
\begin{align}
  \text{ETI}(\mathcal{I}) = \E\left[\#\left\{t \in \mathcal{I} : df(t) = 0\right\} \mid \mathcal{F}\right]\label{eq:ETIdef}
\end{align}
which is the expected value of the size of the random point set of zero crossings by $df$ on $\mathcal{I}$ conditional on a suitable $\sigma$-algebra $\mathcal{F}$. An Expected Trend Instability of zero implies that $f$... The lower $\text{ETI}(\mathcal{I})$ is, the more trend stable $f$ is on $\mathcal{I}$ and vice versa.


## Latent Gaussian Process Model
The definitions in the previous section impose the existence of an infinite dimensional latent variable $f$ and its derivative $df$. In the following we assume that $f$ is a latent Gaussian Process which is equivalent to imposing a functional prior distribution on the latent characteristic [\textbf{TODO: something about flexible}]. Our knowledge about $f$ and $df$ will then be updated based on the observed data giving rise to a joint posterior distribution over $f$ and $df$.

A random function $f$ is a Gaussian Process if and only if the vector $(f(t_1), \ldots, f(t_n))$ has a multivariate normal distribution for every set of evaluation points $(t_1, \ldots, t_n) \subset \mathcal{T}$ with $n < \infty$, and we write $f \sim \mathcal{GP}(\mu(\cdot), C(\cdot, \cdot))$ where $\mu\colon\, \mathcal{T} \mapsto \mathbb{R}$ is the mean function and $C\colon\, \mathcal{T} \times \mathcal{T} \mapsto \mathbb{R}$ is a symmetric, positive definite covariance function [@cramer1967stationary]. Given observed data of the form $(Y_i, t_i)_{i=1}^n$ we lay out the data generating process by the following hierarchical model 
\begin{align}
  f \mid \beta, \theta &\sim \mathcal{GP}(\mu_\beta(\cdot), C_\theta(\cdot,\cdot))\label{eq:generatingProcess}\\
  Y_i \mid t_i, f(t_i), \Theta &\overset{iid}{\sim} N(f(t_i), \sigma^2)\nonumber, \quad \Theta = (\beta, \theta, \sigma)\nonumber
\end{align}
where $\beta$ is a vector of parameters for the mean function of $f$, $\theta$ is a vector of parameters governing the covariance of $f$, and $\sigma^2$ is the observation variance. We assume the following regularity conditions.

\begin{assumption}
We assume that $f$ is a separable Gaussian process and that
\begin{enumerate}
  \item[A1:]{?}
  \item[A2:]{?}
\end{enumerate}
This ensures that the sample paths of $f$ are in $C^r(\mathcal{T})$. We require that $r \geq 1$ for the Trend Direction Index and $r \geq 2$ for the index of Expected Trend Instability.

[@scheuerer2010regularity]
\end{assumption}

Under the above assumptions, an important property of a Gaussian process is that it together with its derivative functions are multivariate Gaussian Process [@cramer1967stationary]. This implies that the joint distribution of $f$ and $df$ is given by the bivariate Gaussian Process
\begin{align}
  \begin{bmatrix}f(s)\\ df(t) \end{bmatrix} \mid \beta, \theta &\sim \mathcal{GP}\left(\begin{bmatrix}\mu_\beta(s)\\ d\mu_\beta(t)\end{bmatrix}, \begin{bmatrix}C_\theta(s, s^\prime) & \partial_2 C_\theta(s, t)\\ \partial_1 C_\theta(t, s) & \partial_1 \partial_2 C_\theta(t, t^\prime)\end{bmatrix}\right)\label{eq:latentJoint}
\end{align}
where $d\mu_\beta$ is the derivative function of $\mu_\beta$ and $\partial_j$ denotes the partial derivatives with respect to the $j$'th variable. This generalizes directly to higher order derivatives assuming that necessary regularity conditions are satisfied. Let $\mathbf{Y} = (Y_1, \ldots, Y_n)$ be the vector of outcomes observed at times $\mathbf{t} = (t_1,\ldots,t_n) \subset \mathbf{T}$ and $\mathbf{t}^\ast$ any finite dimensional vector of time points not necessarily equal to $\mathbf{t}$. Formulas (\ref{eq:generatingProcess}) and (\ref{eq:latentJoint}) imply that joint distribution of $(f(\mathbf{t}^\ast),df(\mathbf{t}^\ast), \mathbf{Y}) \mid \mathbf{t}, \Theta$ is multivariate normal, and that the marginal distribution of the observed data is $\mathbf{Y} \mid \mathbf{t}, \Theta = N(\mu_\beta(\mathbf{t}), C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I)$ where $C_\theta(\mathbf{t}, \mathbf{t})$ denotes a matrix consisting of the values of the covariance function at $\left\{(s,t) \in \mathbf{t} \times \mathbf{t} \right\}$, and $I$ is an identity matrix of appropriate dimensions.  Predictions of the latent function and its derivative at $\mathbf{t}^\ast$ are given by the posterior distribution $(f(\mathbf{t}^\ast),df(\mathbf{t}^\ast)) \mid \mathbf{Y}, \mathbf{t}, \Theta$. This distribution is also multivariate normal and its moments are given in Proposition \ref{prop:GPposterior}.

\begin{proposition}
Let the data generating model be defined as in Equation (\ref{eq:generatingProcess}) and $\mathbf{t}^\ast$ any finite vector of time points. The posterior distribution of $(f, df)$ evaluated at $\mathbf{t}^\ast$ is then given by
\begin{align*}
\begin{bmatrix}f(\mathbf{t}^\ast)\\ df(\mathbf{t}^\ast)\end{bmatrix} \mid \mathbf{Y}, \mathbf{t}, \Theta \sim N\left(\begin{bmatrix}\mu_f(\mathbf{t}^\ast)\\ \mu_{df}(\mathbf{t}^\ast)\end{bmatrix}, \begin{bmatrix}\Sigma_{f}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{f,df}(\mathbf{t}^\ast,\mathbf{t}^\ast)\\ \Sigma_{f,df}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{df}(\mathbf{t}^\ast,\mathbf{t}^\ast)\\ \end{bmatrix}\right)
\end{align*}
where
\begin{align*}
  \bm{\mu}(\mathbf{t}^\ast) &= \begin{bmatrix}\mu_\beta(\mathbf{t}^\ast)\\ d\mu_\beta(\mathbf{t}^\ast)\end{bmatrix} + \begin{bmatrix}C_\theta(\mathbf{t}^\ast, \mathbf{t})\\ \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\end{bmatrix}\left(C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1} \left(\mathbf{Y} - \mu_\beta(\mathbf{t})\right)\\
  \bm{\Sigma}(\mathbf{t}^\ast,\mathbf{t}^\ast) &= \begin{bmatrix}C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast)\\ \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1 \partial_2 C_\theta(\mathbf{t}^\ast,\mathbf{t}^\ast)\\ \end{bmatrix} - \begin{bmatrix}C_\theta(\mathbf{t}^\ast, \mathbf{t})\\\partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\end{bmatrix}\left(C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1}\begin{bmatrix}C_\theta(\mathbf{t}, \mathbf{t}^\ast)\\\partial_2 C_\theta(\mathbf{t}, \mathbf{t}^\ast)\end{bmatrix}
\end{align*}
\label{prop:GPposterior}
\end{proposition}

The results from Proposition \ref{prop:GPposterior} enables us to simulate random realizations from the posterior distribution of $(f, df)$ at any finite number of time points given values of $\Theta$. Further, $(\mu_f, \mu_{df})$ is the Best Linear Unbiased Predictor of $(f, df)$.


## Trend Direction Index
The Trend Direction Index was defined generally in Equation (\ref{eq:TCIdef}). Letting the filtration be the sigma algebra of the observed data and their sampling times, we may express the Trend Direction Index through the posterior distribution of $df$. The following proposition sates this result.

\begin{proposition}
Let $\mathcal{F}_t$ be the sigma algebra generated by $(\mathbf{Y}, \mathbf{t})$. The Trend Direction Index in Equation (\ref{eq:TCIdef}) can then be written in terms of the posterior distribution of $df$ as
\begin{align*}
  \mathrm{TDI}(t, \delta \mid \Theta) &= P(df(t + \delta ) > 0 \mid \mathbf{Y}, \mathbf{t}, \Theta)\\
  &= \int_0^\infty N\left(u; \mu_{df}(t + \delta), \Sigma_{df}(t + \delta, t + \delta)^{1/2}\right)\mathrm{d}u\nonumber
\end{align*}
where $\mu_{df}$ and $\Sigma_{df}$ are given in Proposition \ref{prop:GPposterior} and we condition on the model parameters $\Theta$. 
\label{prop:TDIposterior}
\end{proposition}

We note that the Trend Direction Index depends on the parameters of the model, $\Theta = (\beta, \theta, \sigma^2)$, which in practice must be estimated from data. When taking their uncertainties into account $\text{TDI}$ becomes a functional distribution over probabilities. 

\begin{figure}[htb]
\center\includegraphics{probabilisticExample}
\caption{150 realizations from the posterior distribution of $f$ (top row), $df$ (middle row) with expected values in bold and the Trend Direction Index (bottom row) conditional on one, two and four noise free observations. Dotted vertical lines show the points in time after which forecasting occurs.}
\label{fig:probabilisticExample}
\end{figure}

To illustrate the Trend Direction Index we show an example in the noise free case with known model parameters in Figure \ref{fig:probabilisticExample}. The first and second rows of the plot show 150 random realizations from the posterior distribution of $f$ and $df$ with the expected functions in bold lines, and the third row shows the Trend Direction Index. In this example we simulate using known model parameters and therefore the Trend Direction Index in Proposition \ref{prop:TDIposterior} is a deterministic function. The three columns of the plot show how the posterior of $f$ and $df$ and the Trend Direction Index are updated after one, two or four observed data points. It is seen how updating the Gaussian Process prior with observed data results in a continuous change in the posterior distribution of $f$ and its derivative $df$. The posterior knowledge about the sign of the derivative is correspondingly reflected in the Trend Direction Index. Going further and further away from the last observed data point it is seen that the posterior of $f$ becomes dominated by its prior distribution. In the same sense $df$ converges to a point-wise symmetric distribution around zero and therefore the Trend Direction Index stabilizes around $50\%$. This is clearly seen in the second column of the figure. [\textbf{TODO}: More explaining]

## Expected Trend Instability 
The general form of the Expected Trend Instability index was defined in Equation (\ref{eq:ETIdef}).


\begin{proposition}
The local Expected Trend Instability is
\begin{align*}
  \mathrm{dETI}(t, \mathcal{I} \mid \Theta) = \frac{\Sigma_{d^2\!f}(t,t)^{1/2}}{\Sigma_{df}(t,t)^{1/2}} \left(1 - \omega(t)^2\right)^{1/2} \phi\left(\frac{\mu_{df}(t)}{\Sigma_{df}(t,t)^{1/2}}\right)\left[2\phi(\zeta(t)) + \zeta(t) \Erf\left(\frac{\zeta(t)}{\sqrt{2}}\right)\right]
\end{align*}
where $\phi\colon\, x \mapsto 2^{-1/2}\pi^{-1/2}\exp(-\frac{1}{2}x^2)$ is the standard normal density function, $\Erf\colon\, x \mapsto 2\pi^{-1/2}\int_0^x \exp(-u^2)\mathrm{d}u$ is the error function, and $\omega$ and $\zeta$ are given by
\begin{align*}
  \omega(t) = \frac{\Sigma_{df,d^2\!f}(t,t)}{\Sigma_{df}(t,t)^{1/2}\Sigma_{d^2\!f}(t,t)^{1/2}}, \quad \zeta(t) = \frac{\mu_{d^2\!f}(t) - \mu_{df}(t)\Sigma_{d^2\!f}(t,t)^{1/2}\omega(t) \Sigma_{df}(t,t)^{-1/2}}{\Sigma_{d^2\!f}(t,t)^{1/2}(1-\omega(t)^2)^{1/2}}
\end{align*}
where $\mu_{df}$, $\mu_{d^2\!f}$, $\Sigma_{df}$, $\Sigma_{d^2\!f}$ and $\Sigma_{df,d^2\!f}$ are the posterior moments given in ???.
The Expected Trend Instability is then given by integrating the local Expected Trend Instability over the interval
\begin{align*}
  \mathrm{ETI}(\mathcal{I} \mid \Theta) = \int_\mathcal{I} \mathrm{dETI}(s, \mathcal{I} \mid \Theta)\mathrm{d}s
\end{align*}
\label{prop:ETIposterior}
\end{proposition}

The value of $\text{ETI}$ is a finite [\textbf{TODO:} finite because of assumptions] real positive value.

Figure \ref{fig:ETIexample} shows that Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam vitae vulputate tellus. Cras vitae tempor nibh. Donec dignissim ex ex, nec dignissim magna hendrerit in. Aenean orci nunc, rhoncus quis dictum vitae, tristique non lorem. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Nulla facilisi. Vivamus eget ante consectetur, bibendum nulla non, ultrices risus. Nulla neque mauris, tempus quis dui eu, eleifend gravida sem. Nulla blandit non tortor pellentesque varius.

\begin{figure}[htb]
\center\includegraphics{../figures/ETIexample}
\caption{$25$ random pairs sampled from the joint distribution of a Gassian Process ($f$) and its derivative ($df$) with different values of Expected Trend Instability (ETI). The first row shows samples from $f$, and the second row shows samples from $df$. The columns define different values of ETI. Sample paths that are trend stable are shown by solid blue lines, and unstable sample paths are shown by dashed gold colored lines.}
\label{fig:ETIexample}
\end{figure}



# Estimation {#sec:estimation}
Given observed data $(Y_i, t_i)_{i=1}^n$ there are different approach for estimating models with latent Gaussian Processes. We believe that when doing a probabilistic assessment of a latent variable, such as the Trend Direction Index, it is important to account for all the uncertainty in the model. This is best achieved by using a fully Bayesian estimation. This requires placing prior distributions on the model parameters $\Theta$. These will then be updated based on the observed data and the Trend Direction Index in Proposition \ref{prop:TDIposterior} is then conditional on these posterior distributions. 

[\textbf{TODO:} Write about the covariance function here]

In our application we use the squared exponential covariance function defined in Equation (\ref{eq:seKernel}) which means that our model without the latent class has the following parameters $\Theta = (\beta, \alpha, \rho, \sigma)$. We suggest using the following prior distributions
\begin{align*}
  \beta \overset{iid}{\sim} N(m_\beta, 3), \quad \alpha \sim \text{Half-Normal}(0, 3), \quad \rho \sim \Gamma^{-1}(\rho_a, \rho_b), \quad \sigma \sim \text{Half-Normal}(0, 3)
\end{align*}
where $\text{Half-Normal}$ denotes the half-normal distribution due to the requirement of positivity of $\alpha$ and $\sigma$ and $\Gamma^{-1}$ is the inverse Gamma distribution (\textbf{TODO:} This is not scale-free. Fix). These provide weakly informative priors for the model parameters. We set the hyper-parameter $\mu_m$ in the prior for $m$ as the empirical average of the observed outcomes, $\mu_m = \frac{1}{n}\sum_{i=1}^n Y_i$. The hyper-parameters for the length-scale parameter $\rho$ deserve special attention. The sampling design of the data, i.e., the values of $t_1, \ldots, t_n$, restricts the possible values of estimable length-scales. Specifically, it is impossible to have a length-scale of the latent function smaller than smallest distance between any two sampling times. Similarly, it is impossible to have a length-scale that is greater than the range of the sampling times. To include this information in the prior distribution of $\rho$ while simultaneously not enforcing hard constraints we find the values of $\rho_a$ and $\rho_b$ such that $P(\rho_\text{min} \leq \rho \leq \rho_\text{max}) = 1 - \gamma$ where $\rho_\text{min} = \min_{i,i^\prime} |t_i - t_i^\prime|$, $\rho_\text{max} = \max_{i,i^\prime} |t_i - t_i^\prime|$ and $\gamma$ is a small probability, e.g., $5\%$ or $1\%$. The values of $\rho_a$ and $\rho_b$ can be found using numerical optimization.

We have implemented both our model with and without a latent class in Stan [@carpenter2017stan]. Stan is a probabilistic programming language that enables full Bayesian statistical inference using Markov chain Monte Carlo sampling and can be used together with R through the \texttt{rstan} package [@rstan]. The Stan programs for our two models are available at the first author's Github repository [@gptrendStan].

We ran four chains for 25000 iterations each with half of the iterations used for warm-up. Convergence was assessed by the potential scale reduction factor ($\widehat{R}$) of @gelman1992inference.

\begin{align}
L(\Theta \mid \mathbf{Y}, \mathbf{t}) &=  \frac{1}{2}\log |C_{\theta}(\mathbf{t}, \mathbf{t})| - \frac{1}{2}(\mathbf{Y} - \mu_\beta(\mathbf{t}))C_{\theta}(\mathbf{t}, \mathbf{t})^{-1}(\mathbf{Y} - \mu_\beta(\mathbf{t}))^T\label{eq:margloglik}
\end{align}


# Application {#sec:application}
A report published by The Danish Health Authority in January 2019 updated the estimated proportion of smokers in Denmark with new data from 2018 [@sst]. The data was based on an internet survey including 5017 participants. The report also included data on the proportion of smokers in Denmark during the last 20 years. These data were shown in Figure \ref{fig:rawDataPlot}. The report was picked up by several news papers under headlines stating that the proportion of smokers in Denmark had significantly increased for the first time in two decades [@politiken]. The report published no statistical analyses for this statement but stated in a note that because the study population is so large, then more or less all differences become statistically significant at the $5\%$ level (this was erronously written as a $95\%$ significance level in the report). 

This data set provides an instrumental way of examplifying our two proposed trend quantification measures. In this application we with to assess the statistical properties of following questions:
\begin{itemize}
\item[Q1:]{Is the proportion of smokers increasing in the year 2018 conditional on data from the last 20 years?}
\item[Q2:]{If the proportion of smokers is currently increasing, when did this increase probably start?}
\item[Q3:]{Is it the first time during the last 20 years that the trend in the proportion of smokers has changed?}
\end{itemize}

A simple approach for trying to answer questions Q1 and Q2 would be to apply a $\chi^2$-test in a $2\times 2$ table. Table \ref{tab:chisqtests} shows the p-values for the $\chi^2$-test of independence between the proportion of smokers in 2018 and each of the five previous years. Using a significance level of $5\%$ the conclusion is ambiguous. Compared to the previous year, there was no significant change in the proportion in 2018. Three out of these five comparisons fail to show a significant change in proportions. It is therefore clear that such point-wise testing is not sufficienly perceptive to catch the underlying continuous development.

\begin{table}[htbp]
\center
\begin{tabular}{c|rrrrr}
2018 & 2017 & 2016 & 2015 & 2014 & 2013\\ \hline
p-value & 0.074 & \textbf{0.020} & 0.495 & \textbf{0.012} & 0.576
\end{tabular}
\caption{p-values obtained from $\chi^2$-tests of independence between the proportion of smokers in 2018 and the five previous years. Numbers in bold are significant differences at the $5\%$ level.}
\label{tab:chisqtests}
\end{table}

Similarly, a simple approach for trying to answer question Q3 would be to look at the cumulative number of times that the difference in proportion between consecutive years changes sign. In the data set there were nine changes in the sign of the difference between the proportion in each year and the proportion in the previous year giving this very curde estimate of the number of times, that the trend has changed. This approach suffers from the facts that it is based on a finite difference approximation at the observed time points to the continous derivative, and that it uses the noisy measurements instead of the values of the latent function.


We now present an analysis of the data set using our proposed model. To complete the model specification in Equation (\ref{eq:generatingProcess}) we must decide on the functional forms of the mean and covariance functions of the latent Gaussian Process. We considered three different mean functions: a constant mean, $\mu_\beta(t) = \beta_0$, a linear mean, $\mu_\beta(t) = \beta_0 + \beta_1 t$, and a quadratic mean function, $\mu_\beta(t) = \beta_0 + \beta_1 t + \beta_2 t^2$. For the covariance function we consider the Squared Exponential (SE), the Rational Quadratic (RQ), the Matern 3/2 (M3/2), and the Matern 5/2 (M5/2) covariance functions. The expressions for these covariance functions are given by
\begin{align*}
  C_\theta^\text{SE}(s, t) &= \alpha^2\exp\left(-\frac{(s-t)^2}{2\rho^2}\right), \quad \theta = (\alpha, \rho)\\ 
  C_\theta^\text{RQ}(s, t) &= \alpha^2\left(1 + \frac{(s-t)^2}{2\rho^2\nu}\right)^{-\nu}, \quad \theta = (\alpha, \rho, \nu)\\
  C_\theta^\text{M3/2}(s, t) &= \alpha^2\left(1 + \frac{\sqrt{3}\sqrt{(s-t)^2}}{\rho}\right)\exp\left(-\frac{\sqrt{3}\sqrt{(s-t)^2}}{\rho}\right), \quad \theta = (\alpha, \rho)\\
  C_\theta^\text{M5/2}(s, t) &= \alpha^2\left(1 + \frac{\sqrt{5}\sqrt{(s-t)^2}}{\rho} + \frac{5(s-t)^2}{3\rho^2}\right)\exp\left(-\frac{\sqrt{5}\sqrt{(s-t)^2}}{\rho}\right), \quad \theta = (\alpha, \rho)
\end{align*}
and we note that the Rational Quadractic covariance function converges to the Squared Exponential covariance function for $\nu \rightarrow \infty$, $\lim_{\nu \rightarrow \infty} C_\theta^\text{RQ}(s,t) = C_\theta^\text{SE}(s,t)$. [\textbf{TODO:} Something about the sample path regularity for these covariance functions. M3/2 cannot be used for ETI but okay for TDI]. 

The total combination of mean and covariance functions results in 12 candidate models, and to select the model providing the best fit to data we used leave-one-out cross-validation based on maximum likelihood estimation. For each model, $\mathcal{M}$, we turn by turn excluded a single observation $(t_i, Y_i)$ from the vectors of observed outcomes and time points, and we let the leave-one-out data be denoted by $(\mathbf{Y}_{-i}, \mathbf{t}_{-i})$. Based on the leave-one-out data sets we estimated the parameters,  $\Theta_{-i}^{\mathcal{M}} = (\beta, \theta, \sigma^2)$, of the latent Gaussian Process for each model by maximizing the marginal log-likelihood function of the model in Equation (\ref{eq:generatingProcess}). These estimates were given by
\begin{align*}
\widehat{\Theta}_{-i}^\mathcal{M} = \argsup_{\Theta} L(\Theta \mid \mathbf{Y}_{-i}, \mathbf{t}_{-i}) 
\end{align*}
where the log-likliehood is given by Equation (\ref{eq:margloglik}). These estimates and then plugged into the expression for the posterior expectation of $f$ in Proposition \ref{prop:GPposterior} to obtain the leave-out-out predictions. The mean squared prediction error was then calculated from the squared errors between the leave-one-out predictions and the observed values averaged accross all data points:
\begin{align*}
  \text{MSPE}_{\text{LOO}}^\mathcal{M} = \frac{1}{n}\sum_{i=1}^{n} \left(Y_i - \E[f(t_i) \mid \mathbf{Y}_{-i}, \mathbf{t}_{-i}, \widehat{\Theta}_{-i}^\mathcal{M}]\right)^2
\end{align*}
and the model providing the best fit to data was selected as $\mathcal{M}_{\text{opt}} = \argmin_\mathcal{M} \text{MSPE}_{\text{LOO}}^\mathcal{M}$.

\begin{table}[htbp]
\center
\begin{tabular}{l|rrrr}
 & SE & RQ & Matern 3/2 & Matern 5/2\\ \hline
Constant & 0.682 & \textbf{0.651} & 0.687 & 0.660\\
Linear & 0.806 & $\Leftarrow	$ & 0.896 & 0.865\\
Quadratic & 0.736 & $\Leftarrow$ & 0.800 & 0.785
\end{tabular}
\caption{Leave-one-out cross-validated mean squared error of prediction for each of the 12 candidate models. $\Leftarrow$ indicates numerical convergence to the SE covariance function.}
\label{tab:looTale}
\end{table}

Tabel \ref{tab:looTale} shows the mean squared prediction error for each of the models. For the models with a Rational Quadratic covariance function and a linear and a quadratic mean function the parameter $\nu$ diverged numerically implying convergence to the Squared Exponential covariance function. Comparing the leave-one-out mean squared prediction errors for the candidate models, the optimal model has a constant mean function and a Rational Quadratic covariance function. The marginal maximum likelihood estimates of the parameters of the optimal model was
\begin{align}
  \widehat{\beta_0^\text{ML}} = 28.001, \quad \widehat{\alpha^\text{ML}} = 4.543, \quad \widehat{\rho^\text{ML}} = 4.438, \quad \widehat{\nu^\text{ML}} = 1.020, \quad \widehat{\sigma^\text{ML}} = 0.622
\end{align}

Figure \ref{fig:likFitPlot} shows the fit of the model by plugging in the values of the marginal maximum likelihood estimates into the expressions for the posterior distrubutions of $f$ and $df$ defined in Proposition \ref{prop:GPposterior}, the Trend Direction Index in Proposition \ref{prop:TDIposterior}, and the Local Expected Trend Instability in Proposition \ref{prop:ETIposterior}.

\begin{figure}[htb]
\center\includegraphics{../figures/likFitPlot}
\caption{Results from fitting the latent Gaussian Process model by maximum likelihood. The first row shows the posterior distributions of $f$ (left) and $df$ (right) with the posterior mean in bold and gray areas show point-wise probability intervals for the posterior distribution. The observed data is indicated by points. The second row shows the estimated Trend Direction Index (left) and the local Expected Trend Instability (right).}
\label{fig:likFitPlot}
\end{figure}

The top row of Figure \ref{fig:bayesFitPlot} shows the posterior distributions of the Trend Direction Index and the Local Expected Trend Instability from the Bayesian model. The bottom row shows the posterior distributions of the Expected Trend Instability during the last twenty years and the last ten years.


\begin{figure}[htb]
\center\includegraphics{../figures/bayesFitPlot.pdf}
\caption{hejhej}
\label{fig:bayesFitPlot}
\end{figure}


\begin{table}[htbp]
\center
\begin{tabular}{l|rrl}
  & Maximum Likelihood & \multicolumn{2}{l}{Bayesian Posterior}\\ \hline
$\text{TDI}(2018, 0)$  & 95.24\% & 93.32\% & $[82.15\%; 98.86\%]$\\
$\text{TDI}(2018, -1)$ & 95.92\% & 94.21\% & $[84.28\%; 99.11\%]$\\
$\text{TDI}(2018, -2)$ & 74.41\% & 77.87\% & $[51.02\%; 94.94\%]$\\
$\text{TDI}(2018, -3)$ & 33.36\% & 44.11\% & $[18.23\%; 69.19\%]$\\
$\text{TDI}(2018, -4)$ & 18.96\% & 20.60\% & $[6.05\%; 31.82\%]$\\
$\text{TDI}(2018, -5)$ & 9.50\% & 6.21\% & $[0.03\%; 22.21\%]$\\ \hline
$\text{ETI}([2008, 2018])$ & 1.39 & 1.25 & $[1.02; 2.22]$\\
$\text{ETI}([1998, 2018])$ & 3.68 & 3.36 & $[1.24; 4.79]$\\
\end{tabular}
\caption{Summary measures from the Maximum Likelihood and Bayesian analyses. The rows show the estimated Trend Direction Index for 2013 to 2018 and the Expected Trend Instability for the last 10 and 20 years all conditional on data from 1998 to 2018. For the Bayesian analysis posterior medians and 95\% posterior probability intervals are given.}
\label{tab:summaries}
\end{table}

Cross-point for 50% for MLE: year = 2015.48.

Cross-point for 50% for Bayes: [2014.62; 2015.96]

Posterior 95% CI for $\rho$: $[3.155; 6.206]$

Posterior 95% CI for $\nu$: $[0.328; 10.768]$

# Discussion {#sec:discussion}
[\textbf{TODO:} Her må der båttes løs]

Something about that the indices are scale-free hence TDI should be interpreted along with $df \mid Y$ at the same time point. We could also define
\begin{align*}
  \mathrm{TDI}_u(t, \delta) = P(df(t + \delta ) > u \mid \mathcal{F}_t)
\end{align*}
where $u$ is some trend value.

We could also have defined
\begin{align*}
  \text{ETI}^\uparrow(\mathcal{I}) &= \E\left[\#\left\{t \in \mathcal{I} : df(t) = 0, d^2\!f(t) > 0\right\} \mid \mathcal{F}\right]
\end{align*}
counting the expected number of up-crossings at zero by $df$ (the number of times $f$ has gone from being decreasing to increasing) on $\mathcal{T}$.

Also something about stationarity of $C_\theta$.


#Bibliography
<div id="refs"></div>


#Appendix

## Proof of Proposition \ref{prop:GPposterior}
We derive the joint posterior distribution of the latent process $(f, df, d^2\!f)$ under the model in Equation (\ref{eq:generatingProcess}). We start by noting that $\mathbf{Y} \mid \mathbf{t}, \Theta$ is jointly normal distributed with the latent processes with marginal mean  $\mu_\beta(\mathbf{t})$ and covariance matrix
\begin{align*}
  K(\mathbf{t}, \mathbf{t}) = C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I
\end{align*}
where $C_\theta(\mathbf{t}, \mathbf{t})$ denotes a matrix consisting of the values of the covariance function at $\left\{(s,t) \in \mathbf{t} \times \mathbf{t} \right\}$, and $I$ is an identity matrix of appropriate dimensions. Combining this with the result from Equation (\ref{eq:latentJoint}) generalized to also include the second-order derivative of $f$ we obtain the following joint distribution for the latent processes and the observed data
\begin{align*}
  \begin{bmatrix}f(\mathbf{t}^\ast)\\ df(\mathbf{t}^\ast)\\ d^2\!f(\mathbf{t}^\ast)\\ \mathbf{Y}\end{bmatrix} \mid \mathbf{t}, \Theta \sim N\left(\begin{bmatrix}\mu_\beta(\mathbf{t}^\ast)\\ d\mu_\beta(\mathbf{t}^\ast)\\ d^2\mu_\beta(\mathbf{t}^\ast)\\ \mu_\beta(\mathbf{t})\end{bmatrix}, \begin{bmatrix}C_\theta(\mathbf{t}^\ast,\mathbf{t}^\ast) & \partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & C_\theta(\mathbf{t}^\ast, \mathbf{t})\\ \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) &  \partial_1 \partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1 \partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\\ \partial_1^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1^2\partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1^2\partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1^2 C_\theta(\mathbf{t}^\ast, \mathbf{t})\\ C_\theta(\mathbf{t}, \mathbf{t}^\ast) & \partial_2 C_\theta(\mathbf{t}, \mathbf{t}^\ast) & \partial_2^2 C_\theta(\mathbf{t}, \mathbf{t}^\ast)  & C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\end{bmatrix}\right)
\end{align*}
where $\partial_j^k$ denotes the $k$'th derivative of the $j$'th variable. By the joint normality, the joint posterior distribution of $(f, df, d^2\!f)$ conditional on the parameters and the sampling times is equal to the following multivariate normal distribution
\begin{align*}
\begin{bmatrix}f(\mathbf{t}^\ast)\\ df(\mathbf{t}^\ast)\\ d^2\!f(\mathbf{t}^\ast)\end{bmatrix} \mid \mathbf{Y}, \mathbf{t}, \Theta \sim N\left(\bm{\mu}(\mathbf{t}^\ast), \bm{\Sigma}(\mathbf{t}^\ast, \mathbf{t}^\ast)\right)
\end{align*}
where the moments are partitioned as
\begin{align*}
  \bm{\mu}(\mathbf{t}^\ast) = \begin{bmatrix}\mu_f(\mathbf{t}^\ast)\\ \mu_{df}(\mathbf{t}^\ast)\\ \mu_{d^2\!f}(\mathbf{t}^\ast)\end{bmatrix}, \quad \bm{\Sigma}(\mathbf{t}^\ast,\mathbf{t}^\ast) = \begin{bmatrix}\Sigma_{f}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{f,df}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{f,d^2\!f}(\mathbf{t}^\ast,\mathbf{t}^\ast)\\ \Sigma_{f,df}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{df}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{df,d^2\!f}(\mathbf{t}^\ast,\mathbf{t}^\ast)\\ \Sigma_{d^2\!f,f}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{d^2\!f,df}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{d^2\!f}(\mathbf{t}^\ast,\mathbf{t}^\ast)\end{bmatrix}
\end{align*}
Using the standard formulas for conditional distributions in a multivariate normal model we obtain the following expressions for the joint posterior moments
\begin{align*}
  \bm{\mu}(\mathbf{t}^\ast) &= \begin{bmatrix}\mu_\beta(\mathbf{t}^\ast)\\ d\mu_\beta(\mathbf{t}^\ast)\\ d^2\mu_\beta(\mathbf{t}^\ast)\end{bmatrix} + \begin{bmatrix}C_\theta(\mathbf{t}^\ast, \mathbf{t})\\ \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\\ \partial_1^2 C_\theta(\mathbf{t}^\ast, \mathbf{t})\end{bmatrix}K(\mathbf{t}, \mathbf{t})^{-1} (\mathbf{Y} - \mu_\beta(\mathbf{t}))\\
  \bm{\Sigma}(\mathbf{t}^\ast,\mathbf{t}^\ast) &= \begin{bmatrix}C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast)\\ \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1 \partial_2 C_\theta(\mathbf{t}^\ast,\mathbf{t}^\ast) & \partial_1 \partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast)\\ \partial_1^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1^2 \partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1^2 \partial_2^2 C_\theta(\mathbf{t}^\ast,\mathbf{t}^\ast)\end{bmatrix} - \begin{bmatrix}C_\theta(\mathbf{t}^\ast, \mathbf{t})\\\partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\\ \partial_1^2 C_\theta(\mathbf{t}^\ast, \mathbf{t})\end{bmatrix}K(\mathbf{t}, \mathbf{t})^{-1}\begin{bmatrix}C_\theta(\mathbf{t}, \mathbf{t}^\ast)\\\partial_2 C_\theta(\mathbf{t}, \mathbf{t}^\ast)\\ \partial_2^2 C_\theta(\mathbf{t}, \mathbf{t}^\ast)\end{bmatrix}
\end{align*}

The posterior moments of $f$ are useful for assessing the fit of the model. By completing the expressions in the previous display we obtain these as
\begin{align*}
  \mu_{f}(\mathbf{t}^\ast) &= \mu_\beta(\mathbf{t}^\ast) + C_\theta(\mathbf{t}^\ast, \mathbf{t})\left(C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1}\left(\mathbf{Y} - \mu_\beta(\mathbf{t})\right)\\
  \Sigma_{f}(\mathbf{t}^\ast,\mathbf{t}^\ast) &= C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) - C_\theta(\mathbf{t}^\ast, \mathbf{t})\left(C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1}C_\theta(\mathbf{t}^\ast, \mathbf{t})
\end{align*}
The posterior moments of $df$ are required in order to calculate the Trend Direction Index in Proposition \ref{prop:TDIposterior}, and these are given by
\begin{align}
  \mu_{df}(\mathbf{t}^\ast) &= d\mu_\beta(\mathbf{t}^\ast) + \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\left(C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1}\left(\mathbf{Y} - \mu_\beta(\mathbf{t})\right) \label{eq:appendixPost1}\\
  \Sigma_{df}(\mathbf{t}^\ast,\mathbf{t}^\ast) &= \partial_1 \partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) - \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\left(C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1}\partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}) \nonumber
\end{align}
In order to calculate the Expected Trend Instability index in Proposition \ref{prop:ETIposterior} we in addition require the posterior moments of $d^2\!f$ and its cross-covariance with $df$. These are given by
\begin{align}
  \mu_{d^2\!f}(\mathbf{t}^\ast) &= d^2\mu_\beta(\mathbf{t}^\ast) + \partial_1^2 C_\theta(\mathbf{t}^\ast, \mathbf{t})\left(C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1}\left(\mathbf{Y} - \mu_\beta(\mathbf{t})\right) \label{eq:appendixPost2}\\
  \Sigma_{d^2\!f}(\mathbf{t}^\ast,\mathbf{t}^\ast) &= \partial_1^2 \partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) - \partial_1^2 C_\theta(\mathbf{t}^\ast, \mathbf{t})\left(C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1}\partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}) \nonumber \\ 
  \Sigma_{df, d^2\!f}(\mathbf{t}^\ast,\mathbf{t}^\ast) &= \partial_1 \partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) - \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\left(C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1}\partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}) \nonumber
\end{align}


## Proof of Proposition \ref{prop:ETIposterior}
The generalized Rice formula for the expected number of zero crossings of a Gaussian process $X$ [\textbf{TODO:} Conditions] on an interval $\mathcal{I}$ is 
\begin{align*}
\mathrm{ETI}(\mathcal{I}) &= \int_{\mathcal{I}}\int_{-\infty}^\infty |v|f_{X(t), dX(t)}(0, v)\mathrm{d}v\mathrm{d}t
\end{align*}
where $f_{X(t), dX(t)}$ is the joint density function of $X$ and $dX$ at time $t$ [@rice1944mathematical].

From Proposition \ref{prop:GPposterior} we have that $(df, d^2\!f) \mid \Theta$ follows a bivariate Gaussian process. Under the conditions of [\textbf{TODO:} ??] we may insert the posterior moments of $(df, d^2\!f) \mid \Theta$ and through a direct integration of the expression in the previous display we obtain that the Expected Trend Instability index generally defined in Equation (\ref{eq:ETIdef}) is given by
\begin{align*}
  \mathrm{ETI}(\mathcal{T} \mid \Theta) = \int_{\mathcal{T}} \frac{\Sigma_{d^2\!f}(t,t)^{1/2}}{\Sigma_{df}(t,t)^{1/2}} \sqrt{1 - \omega(t)^2} \phi\left(\frac{\mu_{df}(t)}{\Sigma_{df}(t,t)^{1/2}}\right)\left[2\phi(\eta(t)) + \eta(t) \Erf\left(\frac{\eta(t)}{\sqrt{2}}\right)\right]\mathrm{d}t
\end{align*}
where $\phi(x) = (2\pi)^{-1/2} \exp(-\frac{1}{2}x^2)$ is the standard normal density function, $\Erf(x) = 2\pi^{-1/2}\int_0^x \exp\left(-u^2\right)\mathrm{d}u$ is the error function, and $\omega$ and $\eta$ are functions defined by
\begin{align*}
  \omega(t) = \frac{\Sigma_{df,d^2\!f}(t,t)}{\Sigma_{df}(t,t)^{1/2}\Sigma_{d^2\!f}(t,t)^{1/2}}, \quad \eta(t) = \frac{\mu_{d^2\!f}(t)}{\Sigma_{d^2\!f}(t,t) \sqrt{1-\omega(t)^2}} - \frac{\mu_{df}(t)\omega(t)}{\Sigma_{df}(t,t) \sqrt{1-\omega(t)^2}}
\end{align*}
and $\mu_{df}$, $\mu_{d^2\!f}$, $\Sigma_{df}$, $\Sigma_{d^2\!f}$ and $\Sigma_{df,d^2\!f}$ are defined in Equations (\ref{eq:appendixPost1}) and (\ref{eq:appendixPost2}). [\textbf{TODO:} Check formula for $\eta$]

