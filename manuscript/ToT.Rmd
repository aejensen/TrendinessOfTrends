---
title: "The Trendiness of Trends (or how I learned to stop worrying and love the uncertainty of changes)"
author: | 
  | Andreas Kryger Jensen and Claus Ekstrøm
  | Biostatistics, Institute of Public Health, University of Copenhagen
date: "`r format(Sys.time(), '%d %B, %Y')`"
fontsize: 11pt  
header-includes:
  - \usepackage{bm}
  - \usepackage[labelfont=bf]{caption}
  - \DeclareMathOperator*{\argsup}{arg\,sup}  
  - \DeclareMathOperator*{\E}{E}
  - \DeclareMathOperator*{\Cov}{Cov}
  - \DeclareMathOperator*{\Var}{Var}
  - \DeclareMathOperator*{\Erf}{Erf}
  - \DeclareMathOperator{\Multinomial}{Multinomial}
  - \usepackage[amsthm,thmmarks]{ntheorem}
  - \newtheorem{definition}{Definition}
  - \newtheorem{assumption}{Assumption}
  - \newtheorem{proposition}{Proposition}
output:
  pdf_document: 
    number_sections: yes
    toc: false
bibliography: bibliography.bib    
---


\begin{abstract}
A statement often seen in the news concerning some public health outcome is that some trend has changed or been broken. Such statements are often based on longitudinal data from e.g., surveys, and the change in the trend is claimed to have occurred at the time of the latest data collection. These kinds of statistical assessments are very important as they may influence public health decisions on a national level.

Standard change-point models are not applicable since there are no observations beyond the time of the stipulated change-point. Instead we propose a statistical model using methods from Functional Data Analysis under the assumption that reality evolves in continuous time. Under this assumption we define what constitutes a trend and a change in a trend, and we introduce a probabilistic Trend Direction Index. This index has the intuitive interpretation of the probability that an underlying trend has changed direction at any given time conditional on observed data and can be used for both estimation and forecasting. [\textbf{TODO:} Also mention ETI.]

We show how the Trend Direction Index can be estimated from data in a Bayesian framework and give an application to development of the proportion of smokers in Denmark during the last 20 years. We also show how our model can be extended to include a time-varying latent class variable enabling data-driven adaptation to underlying non-stationarity.
\end{abstract}

\begin{center}
\textbf{Keywords:} Functional Data Analysis, Gaussian Processes, Trends, Bayesian Statistics 
\end{center}

# Introduction
This manuscript is concerned with the fundamental problem of estimating an underlying trend based on random variables observed repeatedly over time. In addition to this problem we also wish to assess points in time where it is possible that such a trend is changing. Our motivation is a recent example from the news in Denmark where it was stated that the trend in the proportion of smokers in Denmark has changed at the end of the year 2018. This statement was based on survey data collected yearly since 1998 and reported by the Danish Health Authority [@sst]. It is not immediately obvious what constitutes a trend and more so what a change in a trend is. To elucidate these questions and provide a clear specification we start from the ground by defining the problem. 

[\textbf{TODO:} Claus båtter sig igennem resten af introduktionen]. 

[\textbf{TODO:} What do we propose? We work in continuous time and introduce TDI and ETI informally at this point. TDI gives the local probability of the direction of $f$ and can answer the question ,,What is the probability that $f$ is increasing?''. ETI gives the expected number of changes in the direction of $f$ on an interval and can answer the question ,,Is it the first time in 20 years that $f$ has changed trend?]. 

\begin{figure}[htb]
\center\includegraphics{rawDataPlot}
\caption{[\textbf{TODO}: add title]}
\label{fig:rawDataPlot}
\end{figure}



[\textbf{TODO:} Reference some previous work]. @gottlieb2012stickiness define a stickiness coefficient for longitudinal data. Our $\text{ETI}$ has some similarities to the tracking index of @foulkes1981index. Empirical dynamics for longitudinal data has been studied by @muller2010empirical.

The manuscript is structured as follows: In section \ref{sec:method} we present our statistical model based on a latent Gaussian Process formulation giving rise to analytic expressions for the Trend Direction Index and the Expected Trend Instability index conditional on observed data. We then extend this basic model to enable a data-driven adaptation to non-stationarity in the underlying dynamics. Section \ref{sec:estimation} is concerned with estimating the models parameters, and we give an application to the proportion of smokers in Denmark during the last 20 years in section \ref{sec:application}, and we conclude with a discussion.



# Methods {#sec:method}
We assume that reality evolves in continuous time $t \in \mathcal{T} \subset \mathbb{R}$ and that there exists a latent, random function $f = \left\{f(t) : t \in \mathcal{T}\right\}$ that governs the underlying evolution of some observable characteristic. We are able to observe a noisy version of this latent characteristic by sampling $f$ at discrete time points according to the additive model $Y_i = f(t_i) + \varepsilon_i$ where $Y_i$ is the $i$'th observation at time $t_i \in \mathcal{T}$ and $\varepsilon_i$ is a random variable with $\E[\varepsilon_i \mid t_i] = 0$ [\textbf{TODO}: Explain why?]. Given observations of the form $(Y_i, t_i)_{i=1}^n$ we are interested in estimating the dynamical properties of the latent $f$.

The trend of $f$ is defined as its instantaneous slope given by the function $df(t) = \left(\frac{\mathrm{d}f(s)}{\mathrm{d}s}\right)(t)$, and we say that $f$ exhibits a positive trend at $t$ if $df(t) > 0$ and a negative trend at $t$ if $df(t) < 0$. A change in trend is defined as a point in time where $df$ changes sign, i.e., a positive trend becomes a negative trend meaning that $f$ goes from being increasing to becoming decreasing or vice versa. As $f$ is a random function there is no exact points in time where $df$ changes sign. This is instead a gradual and continuous concept and the assessment must be defined in terms of the probability that a change in trend is occurring. This stands in contrast to traditional change-point models which assume that there are one or more fixed time points where sudden a change happens (see e.g., @picard1985testing).

To quantify the probability that a trend is changing we introduce the following local probabilistic Trend Direction Index
\begin{align}
  \mathrm{TDI}(t, \delta) = P(df(t + \delta ) > 0 \mid \mathcal{F}_t)\label{eq:TCIdef}
\end{align}
where $\mathcal{F}_t$ is the $\sigma$-algebra of all available information observed up until time $t$. The value of $\mathrm{TDI}(t, \delta)$ is equal to the probability that $f$ is increasing at time $t + \delta$ given all we know about the data generating process up until time $t$. A similar definition could be given for a negative trend but that is equal to $1 - \mathrm{TDI}(t, \delta)$ and therefore redundant. The sign of $\delta$ determines whether we wish to estimate the past ($\delta \leq 0$) or forecast into the future ($\delta > 0$). Most of the examples seen in the news concerning public health outcomes is concerned with $t$ being equal to the current calendar time and $\delta = 0$. This excludes using a standard change-point model as there are no observations beyond the stipulated change-point.

In addition to the Trend Direction Index we also define a global index of trend instability. We say that a function $f$ is \textit{trend stable} on an interval $\mathcal{I} \subseteq \mathcal{T}$ if it remains either increasing or decreasing so that its trend maintains its sign. We quantify the trend instability by the expected number of times $f$ changes direction on the interval, which is equivalent to either counting the expected number local maxima of $f$ or the number of zero crossings by $df$. We define the Expected Trend Instability as
\begin{align}
  \text{ETI}(\mathcal{I}) = \E\left[\#\left\{t \in \mathcal{I} : df(t) = 0\right\} \mid \mathcal{F}\right]\label{eq:ETIdef}
\end{align}
which is the expected value of the size of the random point set of zero crossings by $df$ on $\mathcal{I}$ conditional on a suitable sigma algebra $\mathcal{F}$. The value of $\text{ETI}$ is a finite [\textbf{TODO:} finite because of assumptions] real positive number, and the lower $\text{ETI}(\mathcal{I})$ is, the more trend stable $f$ is on $\mathcal{I}$ and vice versa. [\textbf{TODO:} Maybe something about normalizing by $\mathcal{I}$ or the intepretation.]


## Latent Process Model
The definitions in the previous section impose the existence of an infinite dimensional latent variable $f$ and its derivative $df$. In the following we assume that $f$ is a latent Gaussian Process which is equivalent to imposing a functional prior distribution on the latent characteristic [\textbf{TODO: something about flexible}]. Our knowledge about $f$ and $df$ will then be updated based on the observed data giving rise to a joint posterior distribution over $f$ and $df$.

A random function $f$ is a Gaussian Process if and only if the vector $(f(t_1), \ldots, f(t_n))$ has a multivariate normal distribution for every set of evaluation points $(t_1, \ldots, t_n) \subset \mathcal{T}$ with $n < \infty$, and we write $f \sim \mathcal{GP}(\mu(\cdot), C(\cdot, \cdot))$ where $\mu\colon\, \mathcal{T} \mapsto \mathbb{R}$ is the mean function and $C\colon\, \mathcal{T} \times \mathcal{T} \mapsto \mathbb{R}$ is a symmetric, positive semi-definite covariance function [@cramer1967stationary]. Given observed data of the form $(Y_i, t_i)_{i=1}^n$ we lay out the data generating process by the following hierarchical model 
\begin{align}
  f \mid \beta, \theta &\sim \mathcal{GP}(\mu_\beta(\cdot), C_\theta(\cdot,\cdot))\label{eq:generatingProcess}\\
  Y_i \mid t_i, f(t_i), \Theta &\overset{iid}{\sim} N(f(t_i), \sigma^2)\nonumber, \quad \Theta = (\beta, \theta, \sigma^2)\nonumber
\end{align}
where $\beta$ is a vector of parameters for the mean function of $f$, $\theta$ is a vector of parameters governing the covariance of $f$, and $\sigma^2$ is the observation variance. We assume the following regularity conditions.

\begin{assumption}
We assume that $f$ is a separable Gaussian process and that
\begin{enumerate}
  \item{a}
  \item{b}
\end{enumerate}
This ensures that the sample paths of $f$ are in $C^r(\mathcal{T})$. We require that $r \geq 1$ for the Trend Direction Index and $r \geq 2$ for the index of Expected Trend Instability.

[@scheuerer2010regularity]
\end{assumption}

Under the above assumptions, an important property of a Gaussian process is that it together with its derivative functions are multivariate Gaussian Process [@cramer1967stationary]. This implies that the joint distribution of $f$ and $df$ is given by the bivariate Gaussian Process
\begin{align}
  \begin{bmatrix}f(s)\\ df(t) \end{bmatrix} \mid \beta, \theta &\sim \mathcal{GP}\left(\begin{bmatrix}\mu_\beta(s)\\ d\mu_\beta(t)\end{bmatrix}, \begin{bmatrix}C_\theta(s, s^\prime) & \partial_2 C_\theta(s, t)\\ \partial_1 C_\theta(t, s) & \partial_1 \partial_2 C_\theta(t, t^\prime)\end{bmatrix}\right)\label{eq:latentJoint}
\end{align}
where $d\mu_\beta$ is the derivative function of $\mu_\beta$ and $\partial_j$ denotes the partial derivatives with respect to the $j$'th variable. This generalizes directly to higher order derivatives assuming that necessary regularity conditions are satisfied. Let $\mathbf{Y} = (Y_1, \ldots, Y_n)$ be the vector of outcomes observed at times $\mathbf{t} = (t_1,\ldots,t_n) \subset \mathbf{T}$ and $\mathbf{t}^\ast$ any finite dimensional vector of time points not necessarily equal to $\mathbf{t}$. Formulas (\ref{eq:generatingProcess}) and (\ref{eq:latentJoint}) imply that joint distribution of $(f(\mathbf{t}^\ast),df(\mathbf{t}^\ast), \mathbf{Y}) \mid \mathbf{t}, \Theta$ is multivariate normal, and that the marginal distribution of the observed data is $\mathbf{Y} \mid \mathbf{t}, \Theta = N(\mu_\beta(\mathbf{t}), C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I)$ where $C_\theta(\mathbf{t}, \mathbf{t})$ denotes a matrix consisting of the values of the covariance function at $\left\{(s,t) \in \mathbf{t} \times \mathbf{t} \right\}$, and $I$ is an identity matrix of appropriate dimensions.  Predictions of the latent function and its derivative at $\mathbf{t}^\ast$ are given by the posterior distribution $(f(\mathbf{t}^\ast),df(\mathbf{t}^\ast)) \mid \mathbf{Y}, \mathbf{t}, \Theta$. This distribution is also multivariate normal and its moments are given in Proposition \ref{prop:GPposterior}.

\begin{proposition}
Let the data generating model be defined as in Equation (\ref{eq:generatingProcess}) and $\mathbf{t}^\ast$ any finite vector of time points. The posterior distribution of $(f, df)$ evaluated at $\mathbf{t}^\ast$ is then given by
\begin{align*}
\begin{bmatrix}f(\mathbf{t}^\ast)\\ df(\mathbf{t}^\ast)\end{bmatrix} \mid \mathbf{Y}, \mathbf{t}, \Theta \sim N\left(\begin{bmatrix}\mu_f(\mathbf{t}^\ast)\\ \mu_{df}(\mathbf{t}^\ast)\end{bmatrix}, \begin{bmatrix}\Sigma_{f}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{f,df}(\mathbf{t}^\ast,\mathbf{t}^\ast)\\ \Sigma_{f,df}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{df}(\mathbf{t}^\ast,\mathbf{t}^\ast)\\ \end{bmatrix}\right)
\end{align*}
where
\begin{align*}
  \bm{\mu}(\mathbf{t}^\ast) &= \begin{bmatrix}\mu_\beta(\mathbf{t}^\ast)\\ d\mu_\beta(\mathbf{t}^\ast)\end{bmatrix} + \begin{bmatrix}C_\theta(\mathbf{t}^\ast, \mathbf{t})\\ \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\end{bmatrix}\left[C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right]^{-1} \begin{bmatrix}\mathbf{Y} - \mu_\beta(\mathbf{t})\\ \mathbf{Y} - d\mu_\beta(\mathbf{t})\end{bmatrix}\\
  \bm{\Sigma}(\mathbf{t}^\ast,\mathbf{t}^\ast) &= \begin{bmatrix}C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast)\\ \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1 \partial_2 C_\theta(\mathbf{t}^\ast,\mathbf{t}^\ast)\\ \end{bmatrix} - \begin{bmatrix}C_\theta(\mathbf{t}^\ast, \mathbf{t})\\\partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\end{bmatrix}\left[C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right]^{-1}\begin{bmatrix}C_\theta(\mathbf{t}, \mathbf{t}^\ast)\\\partial_2 C_\theta(\mathbf{t}, \mathbf{t}^\ast)\end{bmatrix}
\end{align*}
\label{prop:GPposterior}
\end{proposition}

The results from Proposition \ref{prop:GPposterior} enables us to simulate random realizations from the posterior distribution of $(f, df)$ at any finite number of time points given values of $\Theta$. Further, $(\mu_f, \mu_{df})$ is the Best Linear Unbiased Predictor of $(f, df)$.


## Trend Direction Index
The Trend Direction Index was defined generally in Equation (\ref{eq:TCIdef}). Letting the filtration be the sigma algebra of the observed data and their sampling times, we may express the Trend Direction Index through the posterior distribution of $df$. The following proposition sates this result.

\begin{proposition}
Let $\mathcal{F}_t$ be the sigma algebra generated by $(\mathbf{Y}, \mathbf{t})$. The Trend Direction Index in Equation (\ref{eq:TCIdef}) can then be written in terms of the posterior distribution of $df$ as
\begin{align*}
  \mathrm{TDI}(t, \delta \mid \Theta) &= P(df(t + \delta ) > 0 \mid \mathbf{Y}, \mathbf{t}, \Theta)\\
  &= \int_0^\infty N\left(u, \mu_{df}(t + \delta), \Sigma_{df}(t + \delta, t + \delta)^{1/2}\right)\mathrm{d}u\nonumber
\end{align*}
where $\mu_{df}$ and $\Sigma_{df}$ are given in Proposition \ref{prop:GPposterior} and we condition on the model parameters $\Theta$. 
\label{prop:TDIposterior}
\end{proposition}

We note that the Trend Direction Index depends on the parameters of the model, $\Theta = (\beta, \theta, \sigma^2)$, which in practice must be estimated from data. When taking their uncertainties into account $\text{TDI}$ becomes a functional distribution over probabilities. 

\begin{figure}[htb]
\center\includegraphics{probabilisticExample}
\caption{150 realizations from the posterior distribution of $f$ (top row), $df$ (middle row) with expected values in bold and the Trend Direction Index (bottom row) conditional on one, two and four noise free observations. Dotted vertical lines show the points in time after which forecasting occurs.}
\label{fig:probabilisticExample}
\end{figure}

To illustrate the Trend Direction Index we show an example in the noise free case with known model parameters in Figure \ref{fig:probabilisticExample}. The first and second rows of the plot show 150 random realizations from the posterior distribution of $f$ and $df$ with the expected functions in bold lines, and the third row shows the Trend Direction Index. In this example we simulate using known model parameters and therefore the Trend Direction Index in Proposition \ref{prop:TDIposterior} is a deterministic function. We have set the mean of the Gaussian Process to $\mu_\beta(t) = 0$ and used the squared exponential (SE) covariance function. [\textbf{TODO:} Move this to estimation] This is an often used covariance function in Gaussian Process regression [@rasmussen2003gaussian] and it has the following form
\begin{align}
  C_\theta(s,t) = \alpha^2 \exp\left(-\frac{(s-t)^2}{2\rho^2}\right), \quad \theta = (\alpha, \rho)\label{eq:seKernel}
\end{align}
The two parameters have the interpretation that $\alpha$ is the standard deviation of the latent function and $\rho$ is its length-scale. For the example we have used $\alpha = 2$ and $\rho = 0.1$ and consider the noise free case with $\sigma^2 = 0$. The three columns of the plot show how the posterior of $f$ and $df$ and the Trend Direction Index are updated after one, two or four observed data points. It is seen how updating the Gaussian Process prior with observed data results in a continuous change in the posterior distribution of $f$ and its derivative $df$. The posterior knowledge about the sign of the derivative is correspondingly reflected in the Trend Direction Index. Going further and further away from the last observed data point it is seen that the posterior of $f$ becomes dominated by its prior distribution. In the same sense $df$ converges to a point-wise symmetric distribution around zero and therefore the Trend Direction Index stabilizes around $50\%$. This is clearly seen in the second column of the figure. [\textbf{TODO}: More explaining]

## Expected Trend Instability 
The general form of the Expected Trend Instability index was defined in Equation (\ref{eq:ETIdef}).

\begin{proposition}
\begin{align*}
  \mathrm{ETI}(\mathcal{T} \mid \Theta) = \int_{\mathcal{T}} \sqrt{\frac{\Sigma_{d^2\!f}(t,t)}{\Sigma_{df}(t,t)}} \sqrt{1 - \rho(t)^2} \phi\left(\frac{\mu_{df}(t)}{\Sigma_{df}(t,t)^{1/2}}\right)\left[2\phi(\eta(t)) + \eta(t) \Erf\left(\frac{\eta(t)}{\sqrt{2}}\right)\right]\mathrm{d}t
\end{align*}
where $\phi$ is the standard normal density function, $\Erf$ is the error function with $\frac{1}{2}\left(\Erf(2^{-1/2} x) + 1\right)$ equal to the standard normal cumulative distribution function, and $\rho$ and $\eta$ are functions defined by
\begin{align*}
  \rho(t) = \frac{\Sigma_{df,d^2\!f}(t,t)}{\sqrt{\Sigma_{df}(t,t)\Sigma_{d^2\!f}(t,t)}}, \quad \eta(t) = \frac{\mu_{d^2\!f}(t)}{\Sigma_{d^2\!f}(t,t) \sqrt{1-\rho(t)^2}} - \frac{\mu_{df}(t)\rho(t)}{\Sigma_{df}(t,t) \sqrt{1-\rho(t)^2}}
\end{align*}
\label{prop:ETIposterior}
\end{proposition}

Figure \ref{fig:ETIexample} shows that Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam vitae vulputate tellus. Cras vitae tempor nibh. Donec dignissim ex ex, nec dignissim magna hendrerit in. Aenean orci nunc, rhoncus quis dictum vitae, tristique non lorem. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Nulla facilisi. Vivamus eget ante consectetur, bibendum nulla non, ultrices risus. Nulla neque mauris, tempus quis dui eu, eleifend gravida sem. Nulla blandit non tortor pellentesque varius.

\begin{figure}[htb]
\center\includegraphics{ETIexample}
\caption{Realizations from the joint distribution of $(f,df)$ using the squared exponential covariance function under different values of the Expected Trend Instability (ETI). Sample paths that are trend stable are shown with blue lines, and unstable sample paths are gold colored.}
\label{fig:ETIexample}
\end{figure}



## Data-driven non-stationarity (better title needed)
By construction, the distribution of the derivative at time $t$ conditional on the value of the function at the same time is a linear function, and the latent dynamics is therefore governed by linear first-order differential equation with time-varying coefficients that can be written as
\begin{align*}
  df(t) \mid f(t), \beta, \theta = a(t) + b(t)f(t) + \epsilon(t)
\end{align*}
where $b(t) = \Cov[df(t), f(t) \mid \beta, \theta]\Cov[f(t), f(t) \mid \beta, \theta]^{-1}$.

[\textbf{TODO:} Fix from here] The generating model described hitherto depends on a parametric covariance function $C_\theta(\cdot, \cdot)$ governing the dynamics of the latent function. Usually in Gaussian Process Regression a stationary covariance function, such as the SE function in Equation (\ref{eq:seKernel}) is chosen out of convenience [@rasmussen2003gaussian]. Choosing a stationary covariance function, however, implies an important model assumption. It can be shown that for all stationary covariance functions $C_{\theta}(s,t) = C_\theta(|s-t|)$, it follows that $\partial_2 C_\theta(t,t) = 0$ and therefore $f(t) \!\perp\!\!\!\perp df(t)$ [@cramer1967stationary].
Using a model with a stationary covariance function implies that $b(t) = 0$, and the model must therefore require non-stationarity in order to have non-trivial latent dynamics with $b(t) \ne 0$.

It can be argued that a priori selecting a parametric non-stationary covariance function that fits data is a very difficult problem. We now present an extension to the previous model that facilitates a data-driven adaptation to non-stationarity in $f$. We do this by introducing a latent time-varying categorical variable $G(t)$ taking values in the discrete set $\left\{1, \ldots, K\right\}$ at every time $t$. We then assume that $f$ conditionally on $G(t) = k$ is a Gaussian Process with a stationary covariance function indexed by a parameter $\theta_k$. This corresponds to the following functional latent class model
\begin{align}
  w(t) \mid \phi &\sim \mathcal{S}_{K-1}^\infty(\phi)\label{eq:latentClassModel}\\
  G(t) \mid w(t), \phi &\sim \Multinomial(w(t))\nonumber\\
  f(t) \mid G(t) = k, \theta_k, w, \phi &\sim \mathcal{GP}(m, C_{\theta_k}(\cdot,\cdot))\nonumber
\end{align}
where $w$ is a $K$-dimensional probability function and $\mathcal{S}_{K-1}^\infty$ is a distribution on the interior of the functional $K-1$ dimensional simplex
\begin{align*}
  \left\{w_k(t), k = 1,\ldots, K, t \in \mathcal{T}, w_k(t) > 0, \sum_{k=1}^K w_k(t) = 1\right\}
\end{align*}
Marginalizing out the distribution of $G(t)$ in $f(t) \mid G(t) = k, \theta_k, w, \phi$ we may write the joint distribution of $f$ and $df$ conditional on the probability weights in the samme form as in Equation (\ref{eq:latentJoint}) but with the following covariance and cross-covariance functions
\begin{align}
  C(s, t) &= \sum_{k=1}^K w_k(s)C_{\theta_k}(s,t)w_k(t)\label{eq:latentClassCov}\\
  \partial_1 C(t, s) &= \sum_{k=1}^K dw_k(s)C_{\theta_k}(s,t)w_k(t) + \sum_{k=1}^K w_k(s) \partial_1 C_{\theta_k}(s,t)w_k(t)\nonumber\\  
  \partial_2 C(s, t) &= \sum_{k=1}^K w_k(s)C_{\theta_k}(s,t)dw_k(t) + \sum_{k=1}^K w_k(s) \partial_2 C_{\theta_k}(s,t)w_k(t)\nonumber\\
  \partial_1\partial_2 C(s, t) &= \sum_{k=1}^K dw_k(s)C_{\theta_k}(s,t)dw_k(t) + \sum_{k=1}^K w_k(s)\partial_1 C_{\theta_k}(s,t)dw_k(t)\nonumber\\
                      &+  \sum_{k=1}^K dw_k(s)\partial_2 C_{\theta_k}(s,t)w_k(t) + \sum_{k=1}^K w_k(s)\partial_1\partial_2 C_{\theta_k}(s,t)w_k(t)\nonumber
\end{align}
The construction with a latent class implies that the covariance function of $f$, $C$, becomes a time-varying convex sum of stationary covariance functions with separate parameters. The expressions for the covariance function of $df$ and its cross-covariance with $f$ follows according to Equation (\ref{eq:latentJoint}) and the chain rule. From the expression of cross-covariance it can be seen that $\partial_2 C(t,t)$ is generally non-zero due to the existence of the terms in the first sum. The terms in the second sum will, however, still equal to zero on the diagonal when each class uses a stationary covariance function. The joint posterior distribution for this extended model is then given similarly to the expressions in Equation (\ref{eq:jointPost}) with the appropriate covariance substitutions.

To model the functional class probabilities, $w(t)$, in Equation (\ref{eq:latentClassModel}) we suggest mapping $K-1$ linear basis expansions onto the simplex. The basis could for example be polynomial splines where we require that the degree is chosen such they are at least once continuously differentiable on $\mathcal{T}$. Let $\left\{B_p(t) : t \in \mathcal{T}, p = 1,\ldots,P\right\}$ be a set of basis functions on $\mathcal{T}$ with $P$ degrees of freedom and $(\phi_0^{(k)}, \ldots, \phi_P^{(k)})$ a vector of coefficients for each $k = 1, \ldots, K-1$ expansion. We then construct the $K-1$ functions as
\begin{align}
  \eta_k(t) = \phi_{0}^{(k)} + \sum_{p=1}^P \phi_{p}^{(k)} B_p(t), \quad k = 1,\ldots, K - 1\label{eq:splineExpansion}
\end{align}
which we map into the class probabilities by the inverse log-additive ratio transform
\begin{align*}
  w_k(t) = \frac{\exp(\eta_k(t))}{1 + \sum_{j=1}^{K-1} \exp(\eta_{j}(t))}, \quad k = 1,\ldots, K-1, \quad w_{K}(t) = \frac{1}{1 + \sum_{j=1}^{K-1} \exp(\eta_j(t))}
\end{align*}
Note that the derivatives of $w_k$ are required in the calculation of the covariance matrices in Equation (\ref{eq:latentClassCov}). These can be derived analytically using the chain rule and pre-computed derivatives of the basis functions.

The parameters of this extension to the model are therefore $\Theta = \left(m, (\phi_0^{(k)}, \ldots, \phi_P^{(k)})_{k=1}^{K-1}, (\theta_k)_{k=1}^K, \sigma^2\right)$ and the Trend Direction Index in Proposition \ref{prop:TDIposterior} is as before defined conditionally on these variables.



# Estimation {#sec:estimation}
Given observed data $(Y_i, t_i)_{i=1}^n$ there are different approach for estimating models with latent Gaussian Processes. We believe that when doing a probabilistic assessment of a latent variable, such as the Trend Direction Index, it is important to account for all the uncertainty in the model. This is best achieved by using a fully Bayesian estimation. This requires placing prior distributions on the model parameters $\Theta$. These will then be updated based on the observed data and the Trend Direction Index in Proposition \ref{prop:TDIposterior} is then conditional on these posterior distributions. 

[\textbf{TODO:} Write about the covariance function here]

In our application we use the squared exponential covariance function defined in Equation (\ref{eq:seKernel}) which means that our model without the latent class has the following parameters $\Theta = (\beta, \alpha, \rho, \sigma)$. We suggest using the following prior distributions
\begin{align*}
  \beta \overset{iid}{\sim} N(m_\beta, 3), \quad \alpha \sim \text{Half-Normal}(0, 3), \quad \rho \sim \Gamma^{-1}(\rho_a, \rho_b), \quad \sigma \sim \text{Half-Normal}(0, 3)
\end{align*}
where $\text{Half-Normal}$ denotes the half-normal distribution due to the requirement of positivity of $\alpha$ and $\sigma$ and $\Gamma^{-1}$ is the inverse Gamma distribution (\textbf{TODO:} This is not scale-free. Fix). These provide weakly informative priors for the model parameters. We set the hyper-parameter $\mu_m$ in the prior for $m$ as the empirical average of the observed outcomes, $\mu_m = \frac{1}{n}\sum_{i=1}^n Y_i$. The hyper-parameters for the length-scale parameter $\rho$ deserve special attention. The sampling design of the data, i.e., the values of $t_1, \ldots, t_n$, restricts the possible values of estimable length-scales. Specifically, it is impossible to have a length-scale of the latent function smaller than smallest distance between any two sampling times. Similarly, it is impossible to have a length-scale that is greater than the range of the sampling times. To include this information in the prior distribution of $\rho$ while simultaneously not enforcing hard constraints we find the values of $\rho_a$ and $\rho_b$ such that $P(\rho_\text{min} \leq \rho \leq \rho_\text{max}) = 1 - \gamma$ where $\rho_\text{min} = \min_{i,i^\prime} |t_i - t_i^\prime|$, $\rho_\text{max} = \max_{i,i^\prime} |t_i - t_i^\prime|$ and $\gamma$ is a small probability, e.g., $5\%$ or $1\%$. The values of $\rho_a$ and $\rho_b$ can be found using numerical optimization.

To estimate the extended model with a latent class we further need to impose prior distributions on the spline coefficients for the probability functions $w_k$. A well-known problem when using spline expansions is selecting the number of knots or equivalently the degrees of freedom of the basis. To mitigate this problem we suggest using a large number of knots and then regularize the splines through the prior distributions on their coefficients. It can be shown that if all spline coefficients are equal, then the resulting expansion is a constant functions. This motivates imposing a random walk prior on the coefficients of each $\eta_k$ in Equation (\ref{eq:splineExpansion}) in order to penalize local variability. For each $k = 1,\ldots, K - 1$ spline expansion we therefore use the following prior distribution
\begin{align*}
  \phi_{0}^{(k)} \sim N(0, 1), \quad \phi_{1}^{(k)} \sim N(0, 1), \quad \{\phi_{p}^{(k)} \sim N(\phi_{p-1}^{(k)}, \tau^{(k)})\}_{p=2}^P, \quad \tau^{(k)} \sim \text{Half-Normal}(0, 1)
\end{align*}

We have implemented both our model with and without a latent class in Stan [@carpenter2017stan]. Stan is a probabilistic programming language that enables full Bayesian statistical inference using Markov chain Monte Carlo sampling and can be used together with R through the \texttt{rstan} package [@rstan]. The Stan programs for our two models are available at the first author's Github repository [@gptrendStan].




# Application {#sec:application}
A report published by The Danish Health Authority in January 2019 updated the estimated proportion of smokers in Denmark with new data from 2018 [@sst]. The data was based on an internet survey including 5017 participants. The report also included data on the proportion of smokers in Denmark during the last 20 years. The result was picked up by several news papers under headlines stating that the proportion of smokers in Denmark had significantly increased for the first time in two decades [@politiken]. The report published no statistical inference on this statement but wrote in a note that because the study population is so large, then more or less all differences become statistically significant at the $95\%$ level. This is an illustrative example of our proposed Trend Direction Index. If it really is so that the proportion of smokers have increased in 2018 we should expect that the Trend Direction Index in $2018$ conditional on data from the last 20 years would have a high value (or at least larger than $50\%$) when fitting our model to this data. 

[\textbf{TODO}: Since the outcomes are proportions calculated from known sample size we know the marginal variance of the outcomes by the formula for the variance of a sample proportion. In the stationary case with a standardized covariance function such that $C_\theta(t, t) = \alpha^2$ we obtain according to Equation (\ref{eq:generatingProcess}) the following equality constraint between the variance of the latent function and the residual variance
\begin{align*}
  \Var[Y_i, \mid t_i, \Theta] = \alpha^2 + \sigma^2 = \frac{Y_i(1-Y_i)}{n_i}
\end{align*}
where $n_i$ is the number of responders at time $t_i$. How do we best enforce this constraint? \textbf{TODO:} Maybe all of this is an unnecceary complication that we don't need to mention.]

\begin{figure}[htb]
\center\includegraphics{smoking1}
\caption{hejhej}
\label{fig:smoking1}
\end{figure}

We fitted the model without a latent class to the data using the Stan implementation with the squared exponential covariance function. We ran four chains for 25000 iterations each with half of the iterations used for warm-up. Convergence was assessed by the potential scale reduction factor ($\widehat{R}$) of @gelman1992inference. The results from the fitted model are seen in Figure \ref{fig:smoking1}.

The upper left panel of Figure \ref{fig:smoking1} shows a plot of the raw data points along with the posterior mean of the latent function as the bold line. The dashed lines are lower and upper $95\%$ probability intervals for posterior distribution of $f$ and the dotted lines are lower and upper $95\%$ probability intervals for the posterior predictive distribution of the proportions. The upper right panel shows $3000$ random realizations from the posterior distribution of $f$ and again the posterior mean as the bold line. The lower left panel shows $3000$ random realizations from the posterior distribution of the derivative of the latent function with the posterior mean as a bold line, at the lower right panel shows $3000$ random realizations from the distribution of the Trend Direction Index along with the mean as the bold line.

The figure shows some interesting things. First, around the year 2006 a increasing trend was also seen even though the average Trend Direction Index was only around $60\%$. This was followed by a long period of close to zero probability of a positive derivative. Secondly, the current reported increase in the proportion of smokers appears to have started already four years ago in 2014 when the average Trend Direction Index started to climb above $50\%$. This peaked around 2016 with a Trend Direction Index of $93\%$. Surprisingly, the Trend Direction Index immediately decreases and in 2018 at the latest survey results it is only $14\%$. This suggests with a high probability that there has been an increase in the proportion of smokers in Denmark but that began four years ago and currently there is no strong evidence for this increase to still occur. The value of the Trend Direction Index in $2018$ even suggests that it is decreasing again.

\begin{figure}[htb]
\center\includegraphics{smoking2-1}
\caption{Median posterior class probability (left) and kernel density estimates of the posterior length-scales (right) for the non-stationary model fitted with two latent classes.}
\label{fig:smoking2-1}
\end{figure}

We also fitted a model with $K = 2$ latent classes to the data. We used a cubic spline basis with $15$ degrees of freedom for the functional class probabilities. The left panel of Figure \ref{fig:smoking2-1} shows the posterior median of the probability that the latent class is equal to class $1$ at any given time. The right panel shows kernel density estimates of the posterior distributions of the length-scale parameter for each latent class. The plot suggests that some degree of non-stationarity is present in the latent dynamics. The latent function seems to develop at two length-scales, one with a posterior mode of $2.78$ and another with a posterior mode of $3.79$. The smaller length-scale has a higher influence during the first ten years with a bump around 2004 corresponding to the close to constant region seen in the upper left panel of Figure \ref{fig:smoking1}. Figure \ref{fig:smoking2-2} shows results similar to Figure \ref{fig:smoking1} but for the latent class model. The conclusions from this model fit are similar to the ones from before.


\begin{figure}[htb]
\center\includegraphics{smoking2-2}
\caption{THIS IS NOT THE CORRECT FIGURE YET (men den er god nok til jazz...)}
\label{fig:smoking2-2}
\end{figure}




# Discussion {#sec:discussion}
[\textbf{TODO:} Her må der båttes løs]


#Bibliography
<div id="refs"></div>


#Appendix

## Proof of Proposition \ref{prop:GPposterior}
We derive the joint posterior distribution of the latent process $(f, df, d^2\!f)$ under the model in Equation (\ref{eq:generatingProcess}). We start by noting that $\mathbf{Y} \mid \mathbf{t}, \Theta$ is jointly normal distributed with the latent processes with marginal mean  $\mu_\beta(\mathbf{t})$ and covariance matrix $K(\mathbf{t}, \mathbf{t}) = C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I$ where $C_\theta(\mathbf{t}, \mathbf{t})$ denotes a matrix consisting of the values of the covariance function at $\left\{(s,t) \in \mathbf{t} \times \mathbf{t} \right\}$, and $I$ is an identity matrix of appropriate dimensions. Combining this with the result from Equation (\ref{eq:latentJoint}) generalized to also include the second-order derivative of $f$ we obtain the following joint distribution for the latent processes and the observed data
\begin{align*}
  \begin{bmatrix}f(\mathbf{t}^\ast)\\ df(\mathbf{t}^\ast)\\ d^2\!f(\mathbf{t}^\ast)\\ \mathbf{Y}\end{bmatrix} \mid \mathbf{t}, \Theta \sim N\left(\begin{bmatrix}\mu_\beta(\mathbf{t}^\ast)\\ d\mu_\beta(\mathbf{t}^\ast)\\ d^2\mu_\beta(\mathbf{t}^\ast)\\ \mu_\beta(\mathbf{t})\end{bmatrix}, \begin{bmatrix}C_\theta(\mathbf{t}^\ast,\mathbf{t}^\ast) & \partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & C_\theta(\mathbf{t}^\ast, \mathbf{t})\\ \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) &  \partial_1 \partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1 \partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\\ \partial_1^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1^2\partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1^2\partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1^2 C_\theta(\mathbf{t}^\ast, \mathbf{t})\\ C_\theta(\mathbf{t}, \mathbf{t}^\ast) & \partial_2 C_\theta(\mathbf{t}, \mathbf{t}^\ast) & \partial_2^2 C_\theta(\mathbf{t}, \mathbf{t}^\ast)  & C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\end{bmatrix}\right)
\end{align*}
where $\partial_j^k$ denotes the $k$'th derivative of the $j$'th variable. By the joint normality, the joint posterior distribution of $(f, df, d^2\!f)$ conditional on the parameters and the sampling times is equal to the following multivariate normal distribution
\begin{align*}
\begin{bmatrix}f(\mathbf{t}^\ast)\\ df(\mathbf{t}^\ast)\\ d^2\!f(\mathbf{t}^\ast)\end{bmatrix} \mid \mathbf{Y}, \mathbf{t}, \Theta \sim N\left(\bm{\mu}(\mathbf{t}^\ast), \bm{\Sigma}(\mathbf{t}^\ast, \mathbf{t}^\ast)\right)
\end{align*}
where the moments are partitioned as
\begin{align*}
  \bm{\mu}(\mathbf{t}^\ast) = \begin{bmatrix}\mu_f(\mathbf{t}^\ast)\\ \mu_{df}(\mathbf{t}^\ast)\\ \mu_{d^2\!f}(\mathbf{t}^\ast)\end{bmatrix}, \quad \bm{\Sigma}(\mathbf{t}^\ast,\mathbf{t}^\ast) = \begin{bmatrix}\Sigma_{f}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{f,df}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{f,d^2\!f}(\mathbf{t}^\ast,\mathbf{t}^\ast)\\ \Sigma_{f,df}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{df}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{df,d^2\!f}(\mathbf{t}^\ast,\mathbf{t}^\ast)\\ \Sigma_{d^2\!f,f}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{d^2\!f,df}(\mathbf{t}^\ast,\mathbf{t}^\ast) & \Sigma_{d^2\!f}(\mathbf{t}^\ast,\mathbf{t}^\ast)\end{bmatrix}
\end{align*}
Using the standard formulas for conditional distributions in a multivariate normal model we obtain the following expressions for the joint posterior moments
\begin{align*}
  \bm{\mu}(\mathbf{t}^\ast) &= \begin{bmatrix}\mu_\beta(\mathbf{t}^\ast)\\ d\mu_\beta(\mathbf{t}^\ast)\\ d^2\mu_\beta(\mathbf{t}^\ast)\end{bmatrix} + \begin{bmatrix}C_\theta(\mathbf{t}^\ast, \mathbf{t})\\ \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\\ \partial_1^2 C_\theta(\mathbf{t}^\ast, \mathbf{t})\end{bmatrix}K(\mathbf{t}, \mathbf{t})^{-1} \begin{bmatrix}\mathbf{Y} - \mu_\beta(\mathbf{t})\\ \mathbf{Y} - d\mu_\beta(\mathbf{t})\\ \mathbf{Y} - d^2\mu_\beta(\mathbf{t})\end{bmatrix}\\
  \bm{\Sigma}(\mathbf{t}^\ast,\mathbf{t}^\ast) &= \begin{bmatrix}C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast)\\ \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1 \partial_2 C_\theta(\mathbf{t}^\ast,\mathbf{t}^\ast) & \partial_1 \partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast)\\ \partial_1^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1^2 \partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) & \partial_1^2 \partial_2^2 C_\theta(\mathbf{t}^\ast,\mathbf{t}^\ast)\end{bmatrix} - \begin{bmatrix}C_\theta(\mathbf{t}^\ast, \mathbf{t})\\\partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\\ \partial_1^2 C_\theta(\mathbf{t}^\ast, \mathbf{t})\end{bmatrix}K(\mathbf{t}, \mathbf{t})^{-1}\begin{bmatrix}C_\theta(\mathbf{t}, \mathbf{t}^\ast)\\\partial_2 C_\theta(\mathbf{t}, \mathbf{t}^\ast)\\ \partial_2^2 C_\theta(\mathbf{t}, \mathbf{t}^\ast)\end{bmatrix}
\end{align*}

The posterior moments of $f$ are useful for assessing the fit of the model. By completing the expressions in the previous display we obtain these as
\begin{align*}
  \mu_{f}(\mathbf{t}^\ast) &= \mu_\beta(\mathbf{t}^\ast) + C_\theta(\mathbf{t}^\ast, \mathbf{t})\left[C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right]^{-1}\left(\mathbf{Y} - \mu_\beta(\mathbf{t})\right)\\
  \Sigma_{f}(\mathbf{t}^\ast,\mathbf{t}^\ast) &= C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) - C_\theta(\mathbf{t}^\ast, \mathbf{t})\left[C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right]^{-1}C_\theta(\mathbf{t}^\ast, \mathbf{t})
\end{align*}
The posterior moments of $df$ are required in order to calculate the Trend Direction Index in Proposition \ref{prop:TDIposterior}, and these are given by
\begin{align}
  \mu_{df}(\mathbf{t}^\ast) &= d\mu_\beta(\mathbf{t}^\ast) + \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\left[C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right]^{-1}\left(\mathbf{Y} - d\mu_\beta(\mathbf{t})\right) \label{eq:appendixPost1}\\
  \Sigma_{df}(\mathbf{t}^\ast,\mathbf{t}^\ast) &= \partial_1 \partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) - \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\left[C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right]^{-1}\partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}) \nonumber
\end{align}
In order to calculate the Expected Trend Instability index in Proposition \ref{prop:ETIposterior} we in addition require the posterior moments of $d^2\!f$ and its cross-covariance with $df$. These are given by
\begin{align}
  \mu_{d^2\!f}(\mathbf{t}^\ast) &= d^2\mu_\beta(\mathbf{t}^\ast) + \partial_1^2 C_\theta(\mathbf{t}^\ast, \mathbf{t})\left[C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right]^{-1}\left(\mathbf{Y} - d^2\mu_\beta(\mathbf{t})\right) \label{eq:appendixPost2}\\
  \Sigma_{d^2\!f}(\mathbf{t}^\ast,\mathbf{t}^\ast) &= \partial_1^2 \partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) - \partial_1^2 C_\theta(\mathbf{t}^\ast, \mathbf{t})\left[C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right]^{-1}\partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}) \nonumber \\ 
  \Sigma_{df, d^2\!f}(\mathbf{t}^\ast,\mathbf{t}^\ast) &= \partial_1 \partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) - \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\left[C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right]^{-1}\partial_2^2 C_\theta(\mathbf{t}^\ast, \mathbf{t}) \nonumber
\end{align}


## Proof of Proposition \ref{prop:ETIposterior}
The generalized Rice formula for the expected number of zero crossings of a Gaussian process $X$ [\textbf{TODO:} Conditions] on an interval $\mathcal{I}$ is 
\begin{align*}
\mathrm{ETI}(\mathcal{I}) &= \int_{\mathcal{I}}\int_{-\infty}^\infty |v|f_{X(t), dX(t)}(0, v)\mathrm{d}v\mathrm{d}t
\end{align*}
where $f_{X(t), dX(t)}$ is the joint density function of $X$ and $dX$ at time $t$ [@rice1944mathematical].

From Proposition \ref{prop:GPposterior} we have that $(df, d^2\!f) \mid \Theta$ follows a bivariate Gaussian process. Under the conditions of [\textbf{TODO:} ??] we may insert the posterior moments of $(df, d^2\!f) \mid \Theta$ and through a direct integration of the expression in the previous display we obtain that the Expected Trend Instability index generally defined in Equation (\ref{eq:ETIdef}) is given by
\begin{align*}
  \mathrm{ETI}(\mathcal{T} \mid \Theta) = \int_{\mathcal{T}} \frac{\Sigma_{d^2\!f}(t,t)^{1/2}}{\Sigma_{df}(t,t)^{1/2}} \sqrt{1 - \omega(t)^2} \phi\left(\frac{\mu_{df}(t)}{\Sigma_{df}(t,t)^{1/2}}\right)\left[2\phi(\eta(t)) + \eta(t) \Erf\left(\frac{\eta(t)}{\sqrt{2}}\right)\right]\mathrm{d}t
\end{align*}
where $\phi$ is the standard normal density function, $\Erf$ is the error function with $\frac{1}{2}\left(\Erf(2^{-1/2} x) + 1\right)$ equal to the standard normal cumulative distribution function, $\rho$ and $\eta$ are functions defined by
\begin{align*}
  \omega(t) = \frac{\Sigma_{df,d^2\!f}(t,t)}{\Sigma_{df}(t,t)^{1/2}\Sigma_{d^2\!f}(t,t)^{1/2}}, \quad \eta(t) = \frac{\mu_{d^2\!f}(t)}{\Sigma_{d^2\!f}(t,t) \sqrt{1-\omega(t)^2}} - \frac{\mu_{df}(t)\omega(t)}{\Sigma_{df}(t,t) \sqrt{1-\omega(t)^2}}
\end{align*}
and $\mu_{df}$, $\mu_{d^2\!f}$, $\Sigma_{df}$, $\Sigma_{d^2\!f}$ and $\Sigma_{df,d^2\!f}$ are defined in Equations (\ref{eq:appendixPost1}) and (\ref{eq:appendixPost2}).



## Remove later
\begin{proposition}
Let the data generating model be defined as in Equation (\ref{eq:generatingProcess}) and $\mathbf{t}^\ast$ any finite vector of time points. Then the posterior distribution of $(f, df)$ at $\mathbf{t}^\ast$ is given by
\begin{align*}
\begin{bmatrix}f(\mathbf{t}^\ast)\\ df(\mathbf{t}^\ast) \end{bmatrix} \mid \mathbf{Y}, \mathbf{t}, \Theta &\sim
N\left(\begin{bmatrix}\mu_f(\mathbf{t}^\ast)\\ \mu_{df}(\mathbf{t}^\ast)\end{bmatrix}, \begin{bmatrix}\Sigma_{f,f}(\mathbf{t}^\ast, \mathbf{t}^\ast) & \Sigma_{f,df}(\mathbf{t}^\ast, \mathbf{t}^\ast)\\ \Sigma_{f,df}(\mathbf{t}^\ast, \mathbf{t}^\ast)^T & \Sigma_{df,df}(\mathbf{t}^\ast, \mathbf{t}^\ast)\end{bmatrix}\right)\\
  \mu_f(\mathbf{t}^\ast) &= m + C_\theta(\mathbf{t}^\ast, \mathbf{t})\left[C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right]^{-1}\left(\mathbf{Y} - m\right)\nonumber\\
  \mu_{df}(\mathbf{t}^\ast) &= \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\left[C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right]^{-1}\mathbf{Y}\nonumber\\
  \Sigma_{f,f}(\mathbf{t}^\ast, \mathbf{t}^\ast) &= C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) - C_\theta(\mathbf{t}^\ast, \mathbf{t})\left[C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right]^{-1} C_\theta(\mathbf{t}^\ast, \mathbf{t})^T\nonumber\\
  \Sigma_{df,df}(\mathbf{t}^\ast, \mathbf{t}^\ast) &= \partial_1 \partial_2 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) - \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})\left[C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right]^{-1} \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})^T\nonumber\\
  \Sigma_{f,df}(\mathbf{t}^\ast, \mathbf{t}^\ast) &= \partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t}^\ast) - C_\theta(\mathbf{t}^\ast, \mathbf{t})\left[C_\theta(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right]^{-1}\partial_1 C_\theta(\mathbf{t}^\ast, \mathbf{t})^T\nonumber
\end{align*}
\end{proposition}